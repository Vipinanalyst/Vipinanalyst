pip install pandas pyreadstat

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, precision_score, recall_score

# Read the .sav file using pandas
df = pd.read_spss('total VNL DATA.sav')

# Display the DataFrame
df.head()

# Dropping the unnecessary column
df = df.drop('NO',axis=1)

#Renaming the column
df = df.rename(columns={'ERROR': 'OPPONENT_ERROR'})

df.info()

#Mapping the category variable into integers
df['RESULT'] = df['RESULT'].map({'won': 1, 'loss': 0})

#Checking if there is any missing values
df.isna().sum()

# Filter DataFrame for wins and losses
df_win = df[df['RESULT'] == 1]
df_loss = df[df['RESULT'] == 0]

# Calculate percentages for wins
total_points_win = df_win['ATTACK'].sum() + df_win['OPPONENT_ERROR'].sum() + df_win['BLOCK'].sum() + df_win['SERVE'].sum()
attack_percentage_win = (df_win['ATTACK'].sum() / total_points_win) * 100
error_percentage_win = (df_win['OPPONENT_ERROR'].sum() / total_points_win) * 100
block_percentage_win = (df_win['BLOCK'].sum() / total_points_win) * 100
serve_percentage_win = (df_win['SERVE'].sum() / total_points_win) * 100

# Calculate percentages for losses
total_points_loss = df_loss['ATTACK'].sum() + df_loss['OPPONENT_ERROR'].sum() + df_loss['BLOCK'].sum() + df_loss['SERVE'].sum()
attack_percentage_loss = (df_loss['ATTACK'].sum() / total_points_loss) * 100
error_percentage_loss = (df_loss['OPPONENT_ERROR'].sum() / total_points_loss) * 100
block_percentage_loss = (df_loss['BLOCK'].sum() / total_points_loss) * 100
serve_percentage_loss = (df_loss['SERVE'].sum() / total_points_loss) * 100

# Pie chart for wins
labels = ['ATTACK', 'OPPONENT_ERROR', 'BLOCK', 'SERVE']
sizes_win = [attack_percentage_win, error_percentage_win, block_percentage_win, serve_percentage_win]

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.pie(sizes_win, labels=labels, autopct='%.2f%%', startangle=140)
plt.title('Percentage of Actions for Win')

# Pie chart for losses
sizes_loss = [attack_percentage_loss, error_percentage_loss, block_percentage_loss, serve_percentage_loss]

plt.subplot(1, 2, 2)
plt.pie(sizes_loss, labels=labels, autopct='%.2f%%', startangle=140)
plt.title('Percentage of Skills for Loss')

plt.tight_layout()
plt.show()

# Inference
# The data indicates that while attacks contribute significantly to points in both winning and losing scenarios,
# the team that lost actually had a higher percentage of attacks and opponent errors. However,
# the main decisive factors in terms of winning appear to be serve and block. There was a difference of
# 1.69% for serves and 1.25% for blocks between the winning and losing teams, which suggests that effective
# serving and blocking were major contributors to the winning team's success. This highlights the
# importance of strategic serving and solid defensive plays in securing victory in volleyball matches.

scaler = MinMaxScaler()

# Fit and transform the numerical columns in the DataFrame
df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
df_normalized.head()

features = ['ATTACK', 'BLOCK', 'SERVE', 'OPPONENT_ERROR']
corr = df_normalized[features].corr()

# Plotting the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr, annot=True, cmap='mako', fmt=".2f", square=True)
plt.title('Correlation Heatmap of Attack, Block, Serve, and Error')
plt.show()

# Inference
# The strongest correlation was observed between Attack and Opponent Error.
# This implies that when teams execute better attacks, it increases the likelihood of opponents committing errors.
# In other words, the quality of attacks influences the frequency of opponent mistakes, indicating a strategic
# advantage for teams with more effective offensive plays.

X = df_normalized.drop('RESULT', axis=1)
y = df_normalized['RESULT']

# Split the data into training and testing sets (80% train, 20% test in this example)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print("Confusion Matrix:")
print(conf_matrix)

mae = mean_absolute_error(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print(f"Mean Absolute Error: {mae:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")

# Calculate F1 score
f1 = f1_score(y_test, y_pred)
print(f"F1 Score: {f1:.2f}")

ppv = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[0, 1]) if (conf_matrix[0, 1] + conf_matrix[1, 1]) != 0 else 0

# Calculate NPV
npv = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[1, 0]) if (conf_matrix[0, 0] + conf_matrix[1, 0]) != 0 else 0

print(f"PPV (Precision): {ppv:.2f}")
print(f"NPV: {npv:.2f}")

# Get the coefficients and feature names
coefficients = model.coef_[0]
feature_names = X.columns

# Create a DataFrame to display coefficients and feature names
coefficients_df = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': coefficients
})

# Sort the DataFrame by the absolute value of coefficients to see the most important features
coefficients_df['Absolute Coefficient'] = coefficients_df['Coefficient'].abs()
sorted_coefficients_df = coefficients_df.sort_values(by='Absolute Coefficient', ascending=False)

# Display the sorted coefficients
print(sorted_coefficients_df)

"""Precision (Precision_score): Of all the instances predicted as "won" (positive), how many were actually "won" (True Positives)?
 A precision of 0.8 means that, among the instances predicted as "won," 80% were correctly predicted.

Recall (Recall_score): Of all the actual instances of "won," how many did the model correctly predict as "won" (True Positives)?
 A recall of 0.6 means that the model captured 60% of the instances of "won."
"""
